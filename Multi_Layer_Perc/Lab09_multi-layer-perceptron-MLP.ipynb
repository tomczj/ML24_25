{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Machine-Learning-2024/blob/master/Lab09_multi-layer-perceptron-MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xl_-W_aXqjJ2",
      "metadata": {
        "id": "xl_-W_aXqjJ2"
      },
      "source": [
        "# Lab 9 - Multi Layer Perceptron, MLP\n",
        "\n",
        "### Author: Szymon Nowakowski"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y88jX_qD1WOy",
      "metadata": {
        "id": "Y88jX_qD1WOy"
      },
      "source": [
        "# Presentation on Tensors and Linear Layers\n",
        "--------------------\n",
        "\n",
        "We shall start off by going through a [short presentation on tensors and linear layers](https://github.com/SzymonNowakowski/Machine-Learning-2024/blob/master/tensors_and_linear_layers.pdf). It is best to first download it and then go through it in a slide-show layout."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nDor-raLgKhg",
      "metadata": {
        "id": "nDor-raLgKhg"
      },
      "source": [
        "# Broadcasting\n",
        "-------------------------------\n",
        "\n",
        "Broadcasting is a powerful feature in PyTorch that allows operations on tensors of different shapes by automatically expanding their dimensions to match each other where possible. This mechanism is essential for performing element-wise operations without needing to manually reshape or replicate data.\n",
        "\n",
        "In broadcasting, PyTorch compares the shapes of the tensors element-wise from the right. If the dimensions are equal or one of them is 1, the operation can proceed. The smaller tensor is virtually expanded (without copying data) to match the larger tensor's shape.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-U867Jmhgd1x",
      "metadata": {
        "id": "-U867Jmhgd1x"
      },
      "source": [
        "\n",
        "## Example in PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "AcdhRfvpgfiV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcdhRfvpgfiV",
        "outputId": "a64e6d49-27a3-4364-f417-586754a39f22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[10., 20., 30.],\n",
            "        [20., 40., 60.],\n",
            "        [30., 60., 90.]])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "A = torch.tensor([[1.], [2.], [3.]])  # Shape: (3, 1)\n",
        "B = torch.tensor([10., 20., 30.])     # Shape: (3) - treated as (1, 3)\n",
        "\n",
        "C = A * B  # Shape: (3, 3) via broadcasting\n",
        "print(C)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "P5St8tIygXbn",
      "metadata": {
        "id": "P5St8tIygXbn"
      },
      "source": [
        "## Note on Numpy\n",
        "The same broadcasting mechanism exists in NumPy, as PyTorch's behavior was designed to be compatible with it:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "U8Vfd3LWgZYf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8Vfd3LWgZYf",
        "outputId": "f17810e6-080e-45aa-ecba-1822d4eefc56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[10. 20. 30.]\n",
            " [20. 40. 60.]\n",
            " [30. 60. 90.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.array([[1.], [2.], [3.]])  # Shape: (3, 1)\n",
        "B = np.array([10., 20., 30.])     # Shape: (3)\n",
        "\n",
        "C = A * B  # Also produces shape (3, 3)\n",
        "print(C)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7EsJu1ctgtig",
      "metadata": {
        "id": "7EsJu1ctgtig"
      },
      "source": [
        "## Note on Previous Class Code\n",
        "\n",
        "We also encountered broadcasting in our previous class session:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "XIKF--QAhI1n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIKF--QAhI1n",
        "outputId": "ed352931-c1f4-4e2f-b4c2-946e25eace96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([100, 1, 2])\n",
            "torch.Size([1, 100, 2])\n",
            "torch.Size([100, 100])\n"
          ]
        }
      ],
      "source": [
        "N = 100\n",
        "focus1 = torch.tensor([-2.0, 0.0])             # First focus,                        sized (2)\n",
        "points = torch.rand((N, 2)) * 10 - 5           # Uniformly distributed in [-5, 5]    sized (N, 2)\n",
        "\n",
        "############### when we computed distances to focal points ##############################\n",
        "dist1 = torch.norm(points - focus1, dim=1)     # the focus 1 gets expanded to (1, 2) then broadcasted to (N, 2)\n",
        "                                               # to match the points tensor\n",
        "\n",
        "                                               # effectively what happens is that\n",
        "                                               # the focus point gets substracted from every other point\n",
        "\n",
        "                                               # also: note how natural this notation feels here\n",
        "\n",
        "############### when we computed pairwise point-to-point distances #####################\n",
        "# Compute all pairwise distances in parallel\n",
        "print(points.unsqueeze(1).size())                   # sized: (N, 1, 2)\n",
        "print(points.unsqueeze(0).size())                   # sized: (1, N, 2)\n",
        "\n",
        "diffs = points.unsqueeze(1) - points.unsqueeze(0)   # sized: (N, N, 2) because of broadcasting\n",
        "distances = torch.norm(diffs, dim=-1)               # sized: (N, N)\n",
        "\n",
        "print(distances.size())                             # N x N\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jTVyxO142fqS",
      "metadata": {
        "id": "jTVyxO142fqS"
      },
      "source": [
        "# Multi Layer Perceptron (MLP) Diagram\n",
        "------------------\n",
        "\n",
        "![network diagram](https://raw.githubusercontent.com/SzymonNowakowski/Machine-Learning-2024/refs/heads/master/MLP_diagram.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "olNkmFP7XR7I",
      "metadata": {
        "id": "olNkmFP7XR7I"
      },
      "source": [
        "# Task\n",
        "-------------------\n",
        "\n",
        "A nonlinear component in-between the linear layers is essential. Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3_fgJXWb5-1C",
      "metadata": {
        "id": "3_fgJXWb5-1C"
      },
      "source": [
        "# A Non-Linearity\n",
        "---------------------\n",
        "\n",
        "$$\n",
        "l_{i+1} = f(W \\cdot l_i),\n",
        "$$\n",
        "with $f$ being a sigmoid $f(\\cdot)=\\sigma(\\cdot)$, hiperbolic tangent $f(\\cdot)=\\tanh(\\cdot)$ or a Rectified Linear Unit (ReLU) $f(\\cdot)=[\\cdot]_+$ (taking the positive part).\n",
        "\n",
        "The **sigmoid function** is defined as:\n",
        "\n",
        "$$\n",
        "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
        "$$\n",
        "\n",
        "It maps any input $x$ to the range $(0,1)$, making it useful for probabilistic interpretations and smooth gradient updates. However, it can suffer from vanishing gradients for large positive or negative inputs.\n",
        "\n",
        "The **hyperbolic tangent (tanh) function** is defined as:\n",
        "\n",
        "$$\n",
        "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
        "$$\n",
        "\n",
        "It maps any input $x$ to the range $(-1,1)$, making it **zero-centered**, which can help with optimization compared to the sigmoid function.\n",
        "\n",
        "Alternatively, **tanh** can be expressed as a **scaled sigmoid**:\n",
        "\n",
        "$$\n",
        "\\tanh(x) = 2\\sigma(2x) - 1\n",
        "$$\n",
        "\n",
        "This shows that **tanh** is just a stretched and shifted version of **sigmoid**, allowing for stronger gradients in the middle range and reducing saturation effects compared to sigmoid.\n",
        "\n",
        "\n",
        "As we said, a non-linear component is essential:\n",
        "1. without it, a compostion of linear components would be just a linear component, so a multilayer network would be equivalent to a single layered network.\n",
        "2. it is a nonlinear component that enables a neural network to express nonlinear functions, too.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fZB1Y2w9qYi",
      "metadata": {
        "id": "4fZB1Y2w9qYi"
      },
      "source": [
        "\n",
        "# MNIST dataset\n",
        "----------------------\n",
        "\n",
        "In this workshop, we will construct an MLP network designed to a specific task of classification of MNIST dataset: a set of handwritten digits from *zero* to *nine*. MNIST stands for Modified National Institute of Standards and Technology database.\n",
        "\n",
        "**You can read more about this dataset [here](https://colah.github.io/posts/2014-10-Visualizing-MNIST/#MNIST).**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wqJVU0gp9rws",
      "metadata": {
        "id": "wqJVU0gp9rws"
      },
      "source": [
        "\n",
        "\n",
        "# Multi Layer Perceptron Notation\n",
        "---------------------------------\n",
        "\n",
        "In this workshop we will be classifying 28 by 28 images into 10 classes. Thus, a four layer perceptron (our classificator) we will work further with can be defined as\n",
        "$\\hat f:\\mathbb{R}^{28\\cdot 28} \\rightarrow \\mathbb{R}^{10}$ defined as\n",
        "\n",
        "$$\\hat f \\left(x; W_1, W_2, W_3, W_4, b_1, b_2, b_3, b_4 \\right) =  W_4 \\left[ W_3 \\left[ W_2 \\left[ W_1 x  + b_1 \\right]_+  + b_2 \\right]_+  + b_3 \\right]_+ + b_4,$$\n",
        "\n",
        "where matrices $W_1, \\ldots, W_4$ are tensors of order two (matrices) with matching dimensions and bias terms $b_1, \\ldots, b_4$ are tensors of order one (vectors) of matching dimensions, and we are using ReLU activation.\n",
        "\n",
        "Note, that there is no nonlinear activation after the last layer in our neural network. **There is an implicit softmax applied while cross entropy loss is calculated by `torch.nn.functional`.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s6RwV0RN_Kc9",
      "metadata": {
        "id": "s6RwV0RN_Kc9"
      },
      "source": [
        "# Reading MNIST Dataset to Play with It\n",
        "--------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "vjAlmExA_Twh",
      "metadata": {
        "id": "vjAlmExA_Twh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from matplotlib import pyplot\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data',\n",
        "                                      train=True,\n",
        "                                      download=True,\n",
        "                                      transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "gcgep60g_Xs6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "gcgep60g_Xs6",
        "outputId": "b1451990-d01c-4752-de25-e49db3f8a68e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_image, train_target = trainset[0]    #let us examine the 0-th sample\n",
        "pyplot.imshow(train_image)\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "5toa5f-l_eUS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5toa5f-l_eUS",
        "outputId": "77bb3c95-ea74-4ecc-c1ed-122ac1dd913d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
              "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
              "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
              "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
              "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
              "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
              "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
              "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
              "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
              "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
              "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
              "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
              "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
              "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
              "       dtype=torch.uint8)"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainset.data[0]     #it will be shown in two rows, so a human has hard time classificating it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "eNgiEB1V_hbV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNgiEB1V_hbV",
        "outputId": "652c4ac9-8b46-4d7a-9d3f-266b8cc37ba1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainset[0][1]    #check if you classified it correctly in your mind"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5kvPHSvuAdDK",
      "metadata": {
        "id": "5kvPHSvuAdDK"
      },
      "source": [
        "# Rereading the MNIST Data (Serious Preparation for Training)\n",
        "------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "Ar2Rzq7UAcjC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar2Rzq7UAcjC",
        "outputId": "68f8f0f2-69c4-44d1-c770-2eac2b0e08f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(np.float64(0.1306604762738429), np.float64(0.30810780385646264))"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(trainset.data.numpy().mean()/255.0, trainset.data.numpy().std()/255.0)   #MNIST datapoints are RGB integers 0-255"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-waIu3OQXGx0",
      "metadata": {
        "id": "-waIu3OQXGx0"
      },
      "source": [
        "# Task\n",
        "---------------\n",
        "\n",
        "Why do we need to normalize the data, and not feed the NN with the 0-255 integers?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "01dayZiEAk3C",
      "metadata": {
        "id": "01dayZiEAk3C"
      },
      "outputs": [],
      "source": [
        "transform = torchvision.transforms.Compose(\n",
        "    [ torchvision.transforms.ToTensor(), #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
        "      torchvision.transforms.Normalize((0.1307), (0.3081))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data',\n",
        "                                      train=True,\n",
        "                                      download=True,\n",
        "                                      transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset,\n",
        "                                          batch_size=2048,\n",
        "                                          shuffle=True)   #we do shuffle it to give more randomizations to training epochs\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data',\n",
        "                                     train=False,\n",
        "                                     download=True,\n",
        "                                     transform=transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset,\n",
        "                                         batch_size=1,\n",
        "                                         shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfqt0kUECnZB",
      "metadata": {
        "id": "dfqt0kUECnZB"
      },
      "source": [
        "## Visualizing Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fxGIwBKGCtFa",
      "metadata": {
        "id": "fxGIwBKGCtFa"
      },
      "source": [
        "### Labels (Ground Truth Outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "bzu3lJ2lCwFq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzu3lJ2lCwFq",
        "outputId": "a7616a56-a698-4f01-bdd0-dae88e7f5b86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 -th batch labels : tensor([4, 4, 2,  ..., 8, 6, 1])\n",
            "1 -th batch labels : tensor([2, 8, 8,  ..., 3, 7, 6])\n",
            "2 -th batch labels : tensor([0, 0, 0,  ..., 3, 5, 2])\n",
            "3 -th batch labels : tensor([7, 5, 9,  ..., 8, 3, 8])\n",
            "4 -th batch labels : tensor([5, 5, 0,  ..., 1, 8, 4])\n"
          ]
        }
      ],
      "source": [
        "for i, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "\n",
        "        if i<5:\n",
        "            print(i, \"-th batch labels :\", batch_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PebIPPfpC05T",
      "metadata": {
        "id": "PebIPPfpC05T"
      },
      "source": [
        "A single label is an entity of order zero (a constant), but batched labels are of order one. The first (and only) index is a sample index within a batch."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NLmKJ1FbC2qJ",
      "metadata": {
        "id": "NLmKJ1FbC2qJ"
      },
      "source": [
        "### Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "2jcRrNw_DQui",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jcRrNw_DQui",
        "outputId": "2d9a5d12-72cd-4a38-a318-c36eb410d137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 -th batch inputs : tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])\n"
          ]
        }
      ],
      "source": [
        "for i, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "\n",
        "        if i==0:\n",
        "            print(i, \"-th batch inputs :\", batch_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YTcC3qH2DZOK",
      "metadata": {
        "id": "YTcC3qH2DZOK"
      },
      "source": [
        "OK, so each data image was initially a two dimensional image when we first saw it, but now the batches have order 4. The first index is a sample index within a batch, but a second index is always 0. This index represents a Channel number inserted here by `ToTensor()` transformation, always 0. As this order is one-dimensional, we can get rid of it, later, in training, in `Flatten` layer or by using `squeeze()` on a tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZX09I7rUDkei",
      "metadata": {
        "id": "ZX09I7rUDkei"
      },
      "source": [
        "# MLP Definition\n",
        "-----------------\n",
        "\n",
        "Your job now is to take the (fully functional) definition of the MLP structure and get rid off the Sequential layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "_jaDXM9oDvrx",
      "metadata": {
        "id": "_jaDXM9oDvrx"
      },
      "outputs": [],
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mlp = torch.nn.Sequential(   #Sequential is a structure which allows stacking layers one on another in such a way,\n",
        "                                          #that output from a preceding layer serves as input to the next layer\n",
        "            torch.nn.Flatten(),   #change the last three orders in data (with dimensions 1, 28 and 28 respectively) into one order of dimensions (1*28*28)\n",
        "            torch.nn.Linear(1*28*28, 1024),  #which is used as INPUT to the first Linear layer\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(1024, 2048),   #IMPORTANT! Please observe, that the OUTPUT dimension of a preceding layer is always equal to the INPUT dimension of the next layer.\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(2048, 256),\n",
        "            torch.nn.ReLU(),            #ReLU (or a Sigmoid if you want) is a nonlinear function which is used in-between layers\n",
        "            torch.nn.Linear(256, 10),\n",
        "        )\n",
        "        self.dropout = torch.nn.Dropout(0.05)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "TSoP6W2UECwb",
      "metadata": {
        "id": "TSoP6W2UECwb"
      },
      "outputs": [],
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.flatten = torch.nn.Flatten()\n",
        "        self.linear1 = torch.nn.Linear(1*28*28, 1024),  #which is used as INPUT to the first Linear layer\n",
        "\n",
        "        # TODO: add more layers\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(0.05)\n",
        "\n",
        "    def forward(self, x):      # B, 1, 28, 28\n",
        "        x = self.flatten(x)    # B, 784\n",
        "\n",
        "        # TODO: add more layers\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fE5rN8UTFFw7",
      "metadata": {
        "id": "fE5rN8UTFFw7"
      },
      "source": [
        "# Training Loop\n",
        "----------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55Sda8Y5FIOh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55Sda8Y5FIOh",
        "outputId": "3a6bba26-fbc4-4d7c-9765-57c93cd3c6d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working on cuda\n",
            "epoch: 0 batch: 0 current batch loss: 2.3052663803100586\n",
            "epoch: 0 batch: 1 current batch loss: 2.091578245162964\n",
            "epoch: 0 batch: 2 current batch loss: 1.6557691097259521\n",
            "epoch: 0 batch: 3 current batch loss: 1.198287010192871\n",
            "epoch: 0 batch: 4 current batch loss: 0.9604135751724243\n",
            "epoch: 0 batch: 5 current batch loss: 1.0911513566970825\n",
            "epoch: 0 batch: 6 current batch loss: 1.0255451202392578\n",
            "epoch: 0 batch: 7 current batch loss: 0.8662630319595337\n",
            "epoch: 0 batch: 8 current batch loss: 0.7674910426139832\n",
            "epoch: 0 batch: 9 current batch loss: 0.5688788890838623\n",
            "epoch: 0 batch: 10 current batch loss: 0.6155306696891785\n",
            "epoch: 0 batch: 11 current batch loss: 0.6007024049758911\n",
            "epoch: 0 batch: 12 current batch loss: 0.5414984822273254\n",
            "epoch: 0 batch: 13 current batch loss: 0.48870036005973816\n",
            "epoch: 0 batch: 14 current batch loss: 0.45891082286834717\n",
            "epoch: 0 batch: 15 current batch loss: 0.4851343333721161\n",
            "epoch: 0 batch: 16 current batch loss: 0.4600960612297058\n",
            "epoch: 0 batch: 17 current batch loss: 0.4203128516674042\n",
            "epoch: 0 batch: 18 current batch loss: 0.36221346259117126\n",
            "epoch: 0 batch: 19 current batch loss: 0.3529694676399231\n",
            "epoch: 0 batch: 20 current batch loss: 0.3760979175567627\n",
            "epoch: 0 batch: 21 current batch loss: 0.33807235956192017\n",
            "epoch: 0 batch: 22 current batch loss: 0.33942118287086487\n",
            "epoch: 0 batch: 23 current batch loss: 0.35271093249320984\n",
            "epoch: 0 batch: 24 current batch loss: 0.3037538528442383\n",
            "epoch: 0 batch: 25 current batch loss: 0.2923465669155121\n",
            "epoch: 0 batch: 26 current batch loss: 0.283556193113327\n",
            "epoch: 0 batch: 27 current batch loss: 0.2659505605697632\n",
            "epoch: 0 batch: 28 current batch loss: 0.2881254255771637\n",
            "epoch: 0 batch: 29 current batch loss: 0.27331191301345825\n",
            "epoch: 1 batch: 0 current batch loss: 0.26461315155029297\n",
            "epoch: 1 batch: 1 current batch loss: 0.2381562739610672\n",
            "epoch: 1 batch: 2 current batch loss: 0.2737581431865692\n",
            "epoch: 1 batch: 3 current batch loss: 0.21289372444152832\n",
            "epoch: 1 batch: 4 current batch loss: 0.22958821058273315\n",
            "epoch: 1 batch: 5 current batch loss: 0.21937069296836853\n",
            "epoch: 1 batch: 6 current batch loss: 0.21958477795124054\n",
            "epoch: 1 batch: 7 current batch loss: 0.23459091782569885\n",
            "epoch: 1 batch: 8 current batch loss: 0.23894456028938293\n",
            "epoch: 1 batch: 9 current batch loss: 0.19150219857692719\n",
            "epoch: 1 batch: 10 current batch loss: 0.1927841454744339\n",
            "epoch: 1 batch: 11 current batch loss: 0.21861632168293\n",
            "epoch: 1 batch: 12 current batch loss: 0.17828890681266785\n",
            "epoch: 1 batch: 13 current batch loss: 0.1562425196170807\n",
            "epoch: 1 batch: 14 current batch loss: 0.17555686831474304\n",
            "epoch: 1 batch: 15 current batch loss: 0.22672924399375916\n",
            "epoch: 1 batch: 16 current batch loss: 0.20010314881801605\n",
            "epoch: 1 batch: 17 current batch loss: 0.18143443763256073\n",
            "epoch: 1 batch: 18 current batch loss: 0.18376831710338593\n",
            "epoch: 1 batch: 19 current batch loss: 0.1660412847995758\n",
            "epoch: 1 batch: 20 current batch loss: 0.1709764450788498\n",
            "epoch: 1 batch: 21 current batch loss: 0.16654446721076965\n",
            "epoch: 1 batch: 22 current batch loss: 0.19455882906913757\n",
            "epoch: 1 batch: 23 current batch loss: 0.16088341176509857\n",
            "epoch: 1 batch: 24 current batch loss: 0.16721627116203308\n",
            "epoch: 1 batch: 25 current batch loss: 0.1807037740945816\n",
            "epoch: 1 batch: 26 current batch loss: 0.18395264446735382\n",
            "epoch: 1 batch: 27 current batch loss: 0.15341654419898987\n",
            "epoch: 1 batch: 28 current batch loss: 0.1466989517211914\n",
            "epoch: 1 batch: 29 current batch loss: 0.1488271951675415\n",
            "epoch: 2 batch: 0 current batch loss: 0.15123197436332703\n",
            "epoch: 2 batch: 1 current batch loss: 0.14225584268569946\n",
            "epoch: 2 batch: 2 current batch loss: 0.14263033866882324\n",
            "epoch: 2 batch: 3 current batch loss: 0.1474236100912094\n",
            "epoch: 2 batch: 4 current batch loss: 0.15233144164085388\n",
            "epoch: 2 batch: 5 current batch loss: 0.13749930262565613\n",
            "epoch: 2 batch: 6 current batch loss: 0.13759452104568481\n",
            "epoch: 2 batch: 7 current batch loss: 0.1346694529056549\n",
            "epoch: 2 batch: 8 current batch loss: 0.13803128898143768\n",
            "epoch: 2 batch: 9 current batch loss: 0.12294110655784607\n",
            "epoch: 2 batch: 10 current batch loss: 0.13096240162849426\n",
            "epoch: 2 batch: 11 current batch loss: 0.13482502102851868\n",
            "epoch: 2 batch: 12 current batch loss: 0.13065914809703827\n",
            "epoch: 2 batch: 13 current batch loss: 0.13851960003376007\n",
            "epoch: 2 batch: 14 current batch loss: 0.12567497789859772\n",
            "epoch: 2 batch: 15 current batch loss: 0.14165203273296356\n",
            "epoch: 2 batch: 16 current batch loss: 0.13085700571537018\n",
            "epoch: 2 batch: 17 current batch loss: 0.13601896166801453\n",
            "epoch: 2 batch: 18 current batch loss: 0.11724407970905304\n",
            "epoch: 2 batch: 19 current batch loss: 0.12430576980113983\n",
            "epoch: 2 batch: 20 current batch loss: 0.11743626743555069\n",
            "epoch: 2 batch: 21 current batch loss: 0.13155075907707214\n",
            "epoch: 2 batch: 22 current batch loss: 0.0877377912402153\n",
            "epoch: 2 batch: 23 current batch loss: 0.11458422988653183\n",
            "epoch: 2 batch: 24 current batch loss: 0.10286147147417068\n",
            "epoch: 2 batch: 25 current batch loss: 0.11534904688596725\n",
            "epoch: 2 batch: 26 current batch loss: 0.0863487496972084\n",
            "epoch: 2 batch: 27 current batch loss: 0.10992155224084854\n",
            "epoch: 2 batch: 28 current batch loss: 0.11600824445486069\n",
            "epoch: 2 batch: 29 current batch loss: 0.10350388288497925\n",
            "epoch: 3 batch: 0 current batch loss: 0.07777460664510727\n",
            "epoch: 3 batch: 1 current batch loss: 0.09753497689962387\n",
            "epoch: 3 batch: 2 current batch loss: 0.08600524067878723\n",
            "epoch: 3 batch: 3 current batch loss: 0.08836802840232849\n",
            "epoch: 3 batch: 4 current batch loss: 0.09131470322608948\n",
            "epoch: 3 batch: 5 current batch loss: 0.11331719905138016\n",
            "epoch: 3 batch: 6 current batch loss: 0.08018289506435394\n",
            "epoch: 3 batch: 7 current batch loss: 0.09219282865524292\n",
            "epoch: 3 batch: 8 current batch loss: 0.0774800106883049\n",
            "epoch: 3 batch: 9 current batch loss: 0.09045518934726715\n",
            "epoch: 3 batch: 10 current batch loss: 0.10261048376560211\n",
            "epoch: 3 batch: 11 current batch loss: 0.10155200958251953\n",
            "epoch: 3 batch: 12 current batch loss: 0.09350930154323578\n",
            "epoch: 3 batch: 13 current batch loss: 0.09126920998096466\n",
            "epoch: 3 batch: 14 current batch loss: 0.08917079120874405\n",
            "epoch: 3 batch: 15 current batch loss: 0.10477301478385925\n",
            "epoch: 3 batch: 16 current batch loss: 0.10165787488222122\n",
            "epoch: 3 batch: 17 current batch loss: 0.09701111912727356\n",
            "epoch: 3 batch: 18 current batch loss: 0.0944337397813797\n",
            "epoch: 3 batch: 19 current batch loss: 0.11318199336528778\n",
            "epoch: 3 batch: 20 current batch loss: 0.08546280115842819\n",
            "epoch: 3 batch: 21 current batch loss: 0.10177400708198547\n",
            "epoch: 3 batch: 22 current batch loss: 0.06915193796157837\n",
            "epoch: 3 batch: 23 current batch loss: 0.08374849706888199\n",
            "epoch: 3 batch: 24 current batch loss: 0.0884498879313469\n",
            "epoch: 3 batch: 25 current batch loss: 0.093259796500206\n",
            "epoch: 3 batch: 26 current batch loss: 0.08766615390777588\n",
            "epoch: 3 batch: 27 current batch loss: 0.10519334673881531\n",
            "epoch: 3 batch: 28 current batch loss: 0.08444616198539734\n",
            "epoch: 3 batch: 29 current batch loss: 0.12813898921012878\n",
            "epoch: 4 batch: 0 current batch loss: 0.06673289090394974\n",
            "epoch: 4 batch: 1 current batch loss: 0.08272901922464371\n",
            "epoch: 4 batch: 2 current batch loss: 0.08207154273986816\n",
            "epoch: 4 batch: 3 current batch loss: 0.06334324926137924\n",
            "epoch: 4 batch: 4 current batch loss: 0.06493064016103745\n",
            "epoch: 4 batch: 5 current batch loss: 0.07568096369504929\n",
            "epoch: 4 batch: 6 current batch loss: 0.07909172773361206\n",
            "epoch: 4 batch: 7 current batch loss: 0.06371866166591644\n",
            "epoch: 4 batch: 8 current batch loss: 0.05827756226062775\n",
            "epoch: 4 batch: 9 current batch loss: 0.07168973237276077\n",
            "epoch: 4 batch: 10 current batch loss: 0.06515873223543167\n",
            "epoch: 4 batch: 11 current batch loss: 0.07662633806467056\n",
            "epoch: 4 batch: 12 current batch loss: 0.07512928545475006\n",
            "epoch: 4 batch: 13 current batch loss: 0.08129389584064484\n",
            "epoch: 4 batch: 14 current batch loss: 0.05630435422062874\n",
            "epoch: 4 batch: 15 current batch loss: 0.06764473021030426\n",
            "epoch: 4 batch: 16 current batch loss: 0.07171734422445297\n",
            "epoch: 4 batch: 17 current batch loss: 0.06069937348365784\n",
            "epoch: 4 batch: 18 current batch loss: 0.062482815235853195\n",
            "epoch: 4 batch: 19 current batch loss: 0.06844095140695572\n",
            "epoch: 4 batch: 20 current batch loss: 0.07119796425104141\n",
            "epoch: 4 batch: 21 current batch loss: 0.064816914498806\n",
            "epoch: 4 batch: 22 current batch loss: 0.06099238991737366\n",
            "epoch: 4 batch: 23 current batch loss: 0.05619579553604126\n",
            "epoch: 4 batch: 24 current batch loss: 0.06548914313316345\n",
            "epoch: 4 batch: 25 current batch loss: 0.07770656049251556\n",
            "epoch: 4 batch: 26 current batch loss: 0.08073779940605164\n",
            "epoch: 4 batch: 27 current batch loss: 0.07509410381317139\n",
            "epoch: 4 batch: 28 current batch loss: 0.06603570282459259\n",
            "epoch: 4 batch: 29 current batch loss: 0.056937769055366516\n",
            "epoch: 5 batch: 0 current batch loss: 0.047926999628543854\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Working on {device}\")\n",
        "\n",
        "net = MLP().to(device)\n",
        "optimizer = torch.optim.Adam(net.parameters(), 0.001)   #initial and fixed learning rate of 0.001.\n",
        "\n",
        "net.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
        "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
        "\n",
        "    for batch, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "\n",
        "        batch_inputs = batch_inputs.to(device)  #explicitly moving the data to the target device\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_outputs = net(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear\n",
        "                                            #and MLP doesn't apply\n",
        "                                            #the nonlinear activation after the last layer\n",
        "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
        "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item())\n",
        "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
        "        optimizer.step()     #but this line in fact updates our neural network.\n",
        "                                ####You can experiment - comment this line and check, that the loss DOE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sTe6ViIsHUbV",
      "metadata": {
        "id": "sTe6ViIsHUbV"
      },
      "source": [
        "# Testing\n",
        "----------------------\n",
        "\n",
        "Correct the code below so it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jcPek-rrHYMi",
      "metadata": {
        "id": "jcPek-rrHYMi"
      },
      "outputs": [],
      "source": [
        "good = 0\n",
        "wrong = 0\n",
        "\n",
        "net.eval()              #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
        "with torch.no_grad():   #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
        "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
        "        datapoint, label = data\n",
        "\n",
        "        prediction = net(datapoint.to(device))                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
        "        classification = torch.argmax(prediction)    #the class is the index of maximal \"prevalence\"\n",
        "\n",
        "        if classification.item() == label.item():\n",
        "            good += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "\n",
        "print(\"accuracy = \", good/(good+wrong))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZJ5c7P6ChB3e",
      "metadata": {
        "id": "ZJ5c7P6ChB3e"
      },
      "source": [
        "# **Homework Assignment - *Do Androids Dream of Electric Sheep?***\n",
        "\n",
        "-------------------------------------  \n",
        "\n",
        "\"Do Androids Dream of Electric Sheep?\"  the famous title of Philip K. Dicks novel  raises a fascinating question: if artificial intelligence could dream, what would it see?  \n",
        "\n",
        "In this assignment, we explore a phenomenon known as **neural network dreams**, where instead of optimizing a neural network's weights, we **optimize the input itself** to achieve a desired classification outcome. Given a fully trained MNIST classification network, your goal is to manipulate its inputs so that it confidently predicts each digit from 0 to 9, starting from pure noise.  \n",
        "\n",
        "## **Tasks Description**  \n",
        "\n",
        "During this class we designed and trained a **MNIST classification neural network**, which takes a **batch of grayscale images** of size **$28 \\times 28$** as input and outputs a probability distribution over the 10 digit classes (09). However, instead of using real MNIST images, you will **treat the input batch itself as a set of trainable parameters** and optimize it so that the network classifies each image as a specific digit.  \n",
        "\n",
        "1. Your first task is to generate **a batch of 10 images**, where each image is\n",
        "   classified as one of the digits **0, 1, 2, ..., 9**, starting from an initial batch of ten random Gaussian noise images.  \n",
        "\n",
        "   Discuss the following question: do the generated images resemble real MNIST digits? Why or why not?  \n",
        "\n",
        "2. Discuss, how you would approach a second task of\n",
        "   generating an image that   \n",
        "   bares similarity to two or more digits simultaneously. **Implement your idea to see the results.**\n",
        "\n",
        "3. Third task: repeat the previous tasks with an additional L2 penalty on noise within the images. Experiment with adding `lambda_l2 * dreamed_input_batch.pow(2).mean()` loss term, with `lambda_l2` being the penalty cooefficient within an exponential progression, say from 0.001 to 10.0. Are the new digits recognized correctly? How does the penalty impact the digit quality? Explain.\n",
        "\n",
        "### **Optimization Process for Task 1**  \n",
        "\n",
        "1. Start with a **batch of 10 random Gaussian noise images** as the initial input and $(0, 1, 2, \\ldots, 9)$ as the expected output batch of target digits.  \n",
        "2. Define the objective: maximize the neural network's confidence for the corresponding target digit for each image in the batch.  \n",
        "3. Use **gradient descent** to modify the pixels in each image, making the network classify each one as the assigned digit.  \n",
        "4. Repeat until the network assigns suffieciently high confidence to each images target class.  \n",
        "\n",
        "### **Implementation Details**  \n",
        "\n",
        "- The neural network weights **must remain frozen** during optimization. You are modifying only the input images.  \n",
        "- The loss function should be the **cross-entropy loss** between the predicted probabilities and the desired class labels (plus an optional weighted L2 penalty regularizing the images in task 3).\n",
        "\n",
        "\n",
        "## **Points to Note**  \n",
        "\n",
        "1. **Visualize** the optimization process: Save images of the generated inputs at different steps and plot the classification confidence evolution over iterations.  \n",
        "3. **Document your findings** and explain the behavior you observe.  \n",
        "\n",
        "## **Task & Deliverables**  \n",
        "\n",
        "- A **Colab notebook** containing solutions for both tasks:\n",
        "  - The full implementation.\n",
        "  - Visualizations of the generated batch of images.\n",
        "  - A written explanation of your observations.\n",
        "- **Bonus:** If you create an **animation** showing the evolution of the input images during optimization, it will be considered a strong enhancement to your submission.\n",
        "  - You can generate an animation programmatically (e.g., using Matplotlib or OpenCV).\n",
        "  - Or, save image frames and use external tools to create a video.\n",
        "  - Provide a **link** to any video files in the README.\n",
        "- Upload your notebook and results to your **GitHub repository** for the course.\n",
        "- In the **README**, include a **link** to the notebook.\n",
        "- In the notebook, include **Open in Colab** badge so it can be launched directly.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
