{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a497636b",
   "metadata": {},
   "source": [
    "# Praca domowa z ML numer 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba5e15",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/tomczj/ML24_25/blob/main/Autoencoders/homework_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26d2ac5",
   "metadata": {},
   "source": [
    "# Transponowana konwolucja - co to jest?\n",
    "\n",
    "Transponowana konwolucja to operacja, która (mówiąc niedokładnie) ma „odwrócić” działanie zwykłej konwolucji w sieciach CNN. Precyzyjniej rzecz ujmując, transponowana konwolucja pozwala odzyskać wcześniejsze wymiary przestrzenne danych. Warto tutaj przypomnieć, że zastosowanie warstw konwolucyjnych zazwyczaj zmniejsza rozmiary przestrzenne danych, stąd potrzeba operacji, która może „odwrócić” ten efekt.\n",
    "\n",
    "Transponowana konwolucja nie jest jednak <strong>przekształceniem odwrotnym</strong> w ścisłym sensie (co zobaczymy), ponieważ jej zastosowanie nie musi (a wręcz zwykle nie pozwala) na odzyskanie dokładnych danych wejściowych.\n",
    "\n",
    "Warto już teraz zaznaczyć, że transponowana konwolucja działa w pewnym sensie odwrotnie do klasycznej konwolucji — z mniejszej liczby wartości „rozlewa” je na większy obszar, zwiększając rozmiary przestrzenne i wytwarzając więcej danych. Dobrze ilustruje to poniższy obrazek, który pozwoliłem sobie wkleić, ponieważ jasno przedstawia ideę i różnice między konwolucją, dekonwolucją oraz transponowaną konwolucją. Warto zaznaczyć, że dekonwolucja ma na celu dokładne odwrócenie działania konwolucji, natomiast transponowana konwolucja może prowadzić do danych innych niż pierwotne dane wejściowe.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/tomczj/ML24_25/main/Autoencoders/cool_diagram.webp\">\n",
    "\n",
    "###### Źródło: https://towardsdatascience.com/what-is-transposed-convolutional-layer-40e5e6e31c11/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325e525",
   "metadata": {},
   "source": [
    "# Jaka jest różnica między zwykłą konwolucją, a konwolucją transponowaną?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed09bbfd",
   "metadata": {},
   "source": [
    "Najlepiej zrozumieć działanie konwolucji transponowanej, zestawiając ją bezpośrednio z klasyczną konwolucją:\n",
    "\n",
    "| Cecha        | Splot (Convolution)                           | Splot transponowany (Transposed Convolution)   |\n",
    "| ------------ | --------------------------------------------- | ---------------------------------------------- |\n",
    "| Działanie    | Zmniejsza rozmiar przestrzenny (downsampling) | Zwiększa rozmiar przestrzenny (upsampling)     |\n",
    "| Wejście      | Duża mapa cech                                | Mniejsza (skompresowana) mapa cech             |\n",
    "| Wyjście      | Mniejsza mapa cech                            | Większa (odtworzona) mapa cech                 |\n",
    "| Obliczenia   | Przesuwa jądro po wejściu                     | Rozprowadza wejście po wyjściu za pomocą jądra |\n",
    "| Zastosowanie | Ekstrakcja cech                               | Rekonstrukcja lub segmentacja                  |\n",
    "\n",
    "Jak wspomniano wcześniej, głównym zastosowaniem konwolucji transponowanych jest rekonstrukcja danych o większej rozdzielczości na podstawie danych o niższej rozdzielczości. Innymi słowy – mając mniejszą mapę cech, możemy dzięki konwolucji transponowanej odpowiednio rozmieścić te wartości w większej przestrzeni, aby uzyskać dane wyjściowe o wyższej rozdzielczości. Jak widać zastosowanie transponowanych konwolucji wręcz się narzuca - na przykład do zwiększenia rozdzielczości zdjęcia, nawet jeśli było ono zapisane wcześniej w słabej jakości.\n",
    "\n",
    "###### Źródło: https://guipleite.medium.com/an-introduction-to-convolutional-neural-networks-for-image-upscaling-using-sub-pixel-cnns-5d9ea911c557"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97070cee",
   "metadata": {},
   "source": [
    "# Jak transponowane konwolucje przeprowadzają upsampling?\n",
    "\n",
    "## Najpierw krótko o składowych, czyli co to stride, padding i rozmiar jądra i na co wpływają.\n",
    "\n",
    "Jak już wspomnieliśmy transponowana konwolucja to coś co ma przypominać przekształcenie odwrotne do konowulcji w CNN. Zamiast jednak odzyskać pełne dane, chcemy odzyskać przede wszystkim wymiar przestrzenny. Najłatwiej będzie to wyjaśnić myśląc o zdjęciach. Załóżmy, że mamy zdjęcie, które zostało zapsiane w słabej rozdzielczości, chcielibyśmy jednak mieć je w większej rozdzielczości, ale jednocześnie zachować jego charakterystykę, to znaczy nie chcemy wstawiać losowych pikseli a te, które najbardziej będą pasować. Przejdźmy teraz do dokładnego opisu. \n",
    "\n",
    "Przyjmijmy następujące wielkości:\n",
    "- `H_in x W_in` - wysokość i szerokość naszego obrazka.\n",
    "- `K x K` - rozmiar jądra jakim będziemy działać na obrazek. Intuicyjnie będzie nam to mówić jak bardzo rozciągamy każdy piksel. Można o tym myśleć, że z każdego pojedyńczego piksela będziemy robić macierz KxK, gdzie wartość liczbową mnożymy właśnie przez macierz wag (czyli jądro).\n",
    "- `S` - stride. Warto już na wstępie zaznaczyć, że jest to inny stride, to znaczy odpowiada za coś innego niż stride w przypadku CNN.\n",
    "- `P` - padding, czyli usunięcie pikseli z brzegów ostatecznego obrazka. Jak zobaczymy nie jest to dokładnie to samo co padding w CNN.\n",
    "\n",
    "Idea jest teraz bardzo prosta, chcemy odtworzyć obrazek o wyższej rozdzielczości odpowiednio manipulując i replikując wartości (pikselami) z obrazka o mniejszej rozdzielczości. Zanim jednak przejdziemy do dokładnego opisu postępowania krok po kroku to warto się zastanowić co robią poszczególne elementy opisane powyżej. \n",
    "\n",
    "#### `Stride` - o co w nim chodzi?\n",
    "\n",
    "W przypadku CNN, stride odpowiadał za to jak bardzo będziemy przesuwać się jądrem po danych (jak duży krok wykonujemy). W przypadku transponowanych konwolucji jest innaczej. Stride określa wypełnienie obrazka jakimiś wartościami (przeważnie 0). To znaczy załóżmy, że mamy pewną macierz 3x3 i zastosujmy na nią różne wartości stride.\n",
    "\n",
    "$$\n",
    "\\begin{array}{cccc}\n",
    "\\textbf{Wejściowa macierz}&\n",
    "\\textbf{Stride = 1} &\n",
    "\\textbf{Stride = 2} &\n",
    "\\textbf{Stride = 3} \\\\\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "&\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "&\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 2 & 0 & 3 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "4 & 0 & 5 & 0 & 6 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "7 & 0 & 8 & 0 & 9\n",
    "\\end{bmatrix}\n",
    "&\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 2 & 0 & 0 & 3 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "4 & 0 & 0 & 5 & 0 & 0 & 6 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "7 & 0 & 0 & 8 & 0 & 0 & 9\n",
    "\\end{bmatrix}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Jak widać, dla danej wartości `stride` = S, wstawiamy S - 1 zer pomiędzy każdą kolumnę i wiersz. Stride jest więc bezpośrednio odpowiedzialny za \"rozciąganie\" wejściowego obrazka.\n",
    "\n",
    "Warto odrazu zauważyć, że zastosowanie stride zwiększa rozmiar obrazka. Załóżmy, że mamy rozmiar `H_in x W_in`. Wówczas zastosowanie stride zwraca obrazek o wymiarach `S(H_in - 1) + 1 x S(W_in - 1) + 1`. Istotnie, wystarczy zliczyć ile zer dodajemy w każdym wymaiarze. Jest to ważne i przyda nam się później jak będziemy chcieli obliczyć wyjściowy wymiar.\n",
    "\n",
    "#### `Padding` - o co w nim chodzi?\n",
    "\n",
    "W przypadku CNN, padding określał ile wartości chcemy dodać na brzegu wejściowych danych zanim zastosujemy jądro. Mogło by się wydawać, że tak samo będzie w przypadku transponowanych konwolucji, ale tak nie jest. W tym przypadku, `padding` jest stosowany dopiero na sam koniec i będzie określał ile wartości chcemy uciąć z brzegu zmodyfikowanego (ostatecznego) obrazka. Idea polega na tym, że po zastosowaniu naszego jądra chcemy mieć dane odpowiedniej wymiarowości. Dokładniej mówiąc odejmujemy `P` pikseli wokół obrazka (z każdego wymiaru, to znaczy z każdej strony), gdzie `P` to własnie nasz padding.\n",
    "\n",
    "Widać, że zastosowanie `paddingu` zmiejsza każdy wymiar o `2P` (bo jednym z \"każdej strony\").\n",
    "\n",
    "#### `Kernel` - o co w nim chodzi?\n",
    "\n",
    "Tutaj wiele się różni względem \"zwykłych\" CNN. `Kernel_size` określa wielkość jądra, którym będziemy działać na obrazek, ale <strong>po zastosowaniu `stride`</strong>. \n",
    "Idea jądra w transponowanych konwolucjach jest znacząco inna względem tego co było w CNN. Ideologicznie można o tym myśleć jak o działaniu, które z jednego piksela robi nowych `KxK`, ale przy zachowaniu tego co wspomnieliśmy już na początku, czyli nie wsadzamy byle jakich pikseli do obrazka, a chcemy takie, które najbardziej pasują, więc mnożymy ten piksel prze pewne wagi (czym właśnie jest nasze jądro). Spróbuje to teraz dokładniej opisać. Na początku bierzemy jeden piskel, a następnie bierzemy kernel, to znaczy w tym przypadku pewną siatkę K x K. Wartości tej siatki zostaną uzupełnione przez wartość naszego piksela pomnożonego przez odpwowiednie wagi - macierz K x K to macierz wag, którą mnożymy przez wartość liczbową piksela.\n",
    "\n",
    "Wobec tego można zauważyć, że duże jądra będą powodowały silniejsze rozymycie informacji, natomiast małe jądra dadzą bardziej skoncentrowaną informację. Tak więc wszytsko zależy od tego jaki cel chcemy osiągnąć. \n",
    "\n",
    "<strong>Bardzo ważne jest to, że w przypadku transponowanych konwolucji nie specyfikujemy kroku jaki jądro będzie wykonywać</strong> - zawsze będzie się ono poruszało z krokiem 1! Należy również poruszyć jedną kwestię. Ze względnu na fakt, że jądro porusza się z krokiem 1 to niektóre piksele się pokryją w wyjściowym obrazku. Wówczas na na rozciągniętych pikselach, gdzie jądro na siebie najdzie będziemy sumować te wartości. Najlepiej widać to na diagramie\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/tomczj/ML24_25/main/Autoencoders/diagram.jpg\" width=\"400\">\n",
    "\n",
    "Możemy teraz policzyć jak będzie wyglądał finalny wymiar przed zastosowaniem paddingu,a po `stride`.  Przyjmijmy, że mamy jądro rozmiaru `K x K`. Wówczas łatwo zauważyć, że do aktualnego wymiaru dodajemy `K-1`. Co za tym idzie po zastosowaniu stride i kernela mamy następujący wymiar obrazka \n",
    "\n",
    "`S(H_in - 1) + 1 + K - 1 x S(W_in - 1) + 1 + K - 1` = `S(H_in - 1) + K x S(W_in - 1) + K`\n",
    "\n",
    "\n",
    "### Szybkie podsumowanie\n",
    "\n",
    "Jak widać, stride, padding oraz kernel_size odpowiadają – odpowiednio – za rozciągnięcie, obcięcie i rozmycie danych (myślimy tu głównie o obrazkach, żeby zachować intuicję). Odpowiednie dobranie tych parametrów pozwala precyzyjnie kontrolować wymiar wyjściowego obrazka, który wiemy jak będzie wyglądał, ponieważ od poprzednich wartości wystarczy po prostu odjąć `2P`. Co za tym idzie końcowy wymiar obrazka będzie wynosił\n",
    "\n",
    "`S(H_in - 1) + K - 2P x S(W_in - 1) + K - 2P`\n",
    "\n",
    "## Jak zatem wygląda proces upsamplingu?\n",
    "\n",
    "Udało się nam już opisać główną ideę. Teraz możemy opsisać algortytm krok po kroku. Tak jak wcześniej przyjmijmy \n",
    "\n",
    "- `H_in x W_in` - wysokość i szerokość naszego obrazka.\n",
    "- `K x K` - rozmiar jądra jakim będziemy działać na obrazek. \n",
    "- `S` - stride. \n",
    "- `P` - padding.\n",
    "\n",
    "<strong>Krok I </strong> - musimy sobie odpowiedzieć na pytanie jakie wymiary chcemy uzyskać.\n",
    "\n",
    "Zaczynamy od ustalenia, jaki rozmiar chcemy uzyskać na wyjściu. W przypadku transponowanej konwolucji, rozmiar wynikowy H_out × W_out możemy wyznaczyć ze wzoru (pokazaliśmy go wcześniej)\n",
    "\n",
    "`H_out = S × (H_in - 1) + K - 2P`\n",
    "`W_out = S × (W_in - 1) + K - 2P`\n",
    "\n",
    "Ten wzór pozwala nam dobrać odpowiednie parametry (kernel, stride, padding), aby otrzymać pożądany rozmiar wyjściowy.\n",
    "\n",
    "<strong>Krok II </strong> - rozmieszczenie zer, czyli `stride`\n",
    "\n",
    "Transponowana konwolucja działa odwrotnie niż zwykła – przed właściwym „splotem” obraz jest rozrzedzany. Między kolejne kolumny i wiersze wejściowej macierzy wstawiamy S - 1 zer - za to odpowiada właśnie stride. W następnym kroku na rozrzedzoną macierz zadziałamy jądrem.\n",
    "\n",
    "<strong>Krok III </strong> - działanie jądrem\n",
    "\n",
    "Używamy jądra K × K, aby z pojedynczego piksela wygenerować fragment o rozmiarze K × K. Oznacza to, że wartość tego piksela zostaje przemnożona przez macierz wag (czyli jądro), a wynik wstawiany do macierzy wynikowej (czyli fragmentu wynikowego obrazka). Następnie przechodzimy do kolejnego piksela w tymczasowej macierzy (z krokiem 1). Jeśli kilka takich fragmentów nakłada się na te same piksele, ich wartości się sumują — jak pokazano na poniższym schemacie.\n",
    "\n",
    "<strong>Krok IV </strong> - padding\n",
    "\n",
    "Na końcu dodajemy padding, czyli wyjściowy obrazek obcinamy - po P pikseli z każdego wymiaru.\n",
    "\n",
    "Ogólnie otrzymujemy schemat (zobrazowany poniżej). W miejscach kropek w macierzy należy wstawić zera:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/tomczj/ML24_25/main/Autoencoders/diagram2.jpg\" width=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35d3553",
   "metadata": {},
   "source": [
    "# Przykłady\n",
    "\n",
    "Teraz omówimy proste przykłady dla różnych wartości paddingu i stride. \n",
    "\n",
    "### Przykład 1\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/tomczj/ML24_25/main/Autoencoders/exp2.jpg\" width=\"600\"/>\n",
    "\n",
    "### Przykład 2\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/tomczj/ML24_25/main/Autoencoders/exp1.jpg\" width=\"600\"/>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
